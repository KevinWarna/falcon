{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9329c89a",
   "metadata": {},
   "source": [
    "# from https://github.com/SeHwanJoo/cifar10-vgg16/blob/master/vgg16.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2f79cc",
   "metadata": {},
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "def normalization(train_images, test_images):\n",
    "    mean = np.mean(train_images, axis=(0, 1, 2, 3))\n",
    "    std = np.std(train_images, axis=(0, 1, 2, 3))\n",
    "    train_images = (train_images - mean) / (std + 1e-7)\n",
    "    test_images = (test_images - mean) / (std + 1e-7)\n",
    "    return train_images, test_images\n",
    "\n",
    "\n",
    "def load_images():\n",
    "    (train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "\n",
    "    train_images = train_images.astype(np.float32)\n",
    "    test_images = test_images.astype(np.float32)\n",
    "\n",
    "    (train_images, test_images) = normalization(train_images, test_images)\n",
    "\n",
    "    train_labels = to_categorical(train_labels, 10)\n",
    "    test_labels = to_categorical(test_labels, 10)\n",
    "\n",
    "    # train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(\n",
    "    #     buffer_size=10000).batch(batch_size)\n",
    "    # test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(batch_size)\n",
    "\n",
    "    return train_images, train_labels, test_images, test_labels\n",
    "\n",
    "\n",
    "class ConvBNRelu(tf.keras.Model):\n",
    "    def __init__(self, filters, kernel_size=3, strides=1, padding='SAME', weight_decay=0.0005, rate=0.4, drop=True):\n",
    "        super(ConvBNRelu, self).__init__()\n",
    "        self.drop = drop\n",
    "        self.conv = keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, strides=strides,\n",
    "                                        padding=padding, kernel_regularizer=tf.keras.regularizers.l2(weight_decay))\n",
    "        self.batchnorm = tf.keras.layers.BatchNormalization()\n",
    "        self.dropOut = keras.layers.Dropout(rate=rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        layer = self.conv(inputs)\n",
    "        layer = tf.nn.relu(layer)\n",
    "        layer = self.batchnorm(layer)\n",
    "        if self.drop:\n",
    "            layer = self.dropOut(layer)\n",
    "\n",
    "        return layer\n",
    "\n",
    "\n",
    "class VGG16Model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(VGG16Model, self).__init__()\n",
    "        self.conv1 = ConvBNRelu(filters=64, kernel_size=[3, 3], rate=0.3)\n",
    "        self.conv2 = ConvBNRelu(filters=64, kernel_size=[3, 3], drop=False)\n",
    "        self.maxPooling1 = keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
    "        self.conv3 = ConvBNRelu(filters=128, kernel_size=[3, 3])\n",
    "        self.conv4 = ConvBNRelu(filters=128, kernel_size=[3, 3], drop=False)\n",
    "        self.maxPooling2 = keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
    "        self.conv5 = ConvBNRelu(filters=256, kernel_size=[3, 3])\n",
    "        self.conv6 = ConvBNRelu(filters=256, kernel_size=[3, 3])\n",
    "        self.conv7 = ConvBNRelu(filters=256, kernel_size=[3, 3], drop=False)\n",
    "        self.maxPooling3 = keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
    "        self.conv11 = ConvBNRelu(filters=512, kernel_size=[3, 3])\n",
    "        self.conv12 = ConvBNRelu(filters=512, kernel_size=[3, 3])\n",
    "        self.conv13 = ConvBNRelu(filters=512, kernel_size=[3, 3], drop=False)\n",
    "        self.maxPooling5 = keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
    "        self.conv14 = ConvBNRelu(filters=512, kernel_size=[3, 3])\n",
    "        self.conv15 = ConvBNRelu(filters=512, kernel_size=[3, 3])\n",
    "        self.conv16 = ConvBNRelu(filters=512, kernel_size=[3, 3], drop=False)\n",
    "        self.maxPooling6 = keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
    "        self.flat = keras.layers.Flatten()\n",
    "        self.dropOut = keras.layers.Dropout(rate=0.5)\n",
    "        self.dense1 = keras.layers.Dense(units=512,\n",
    "                                         activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0005))\n",
    "        self.batchnorm = tf.keras.layers.BatchNormalization()\n",
    "        self.dense2 = keras.layers.Dense(units=10)\n",
    "        self.softmax = keras.layers.Activation('softmax')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        net = self.conv1(inputs)\n",
    "        net = self.conv2(net)\n",
    "        net = self.maxPooling1(net)\n",
    "        net = self.conv3(net)\n",
    "        net = self.conv4(net)\n",
    "        net = self.maxPooling2(net)\n",
    "        net = self.conv5(net)\n",
    "        net = self.conv6(net)\n",
    "        net = self.conv7(net)\n",
    "        net = self.maxPooling3(net)\n",
    "        net = self.conv11(net)\n",
    "        net = self.conv12(net)\n",
    "        net = self.conv13(net)\n",
    "        net = self.maxPooling5(net)\n",
    "        net = self.conv14(net)\n",
    "        net = self.conv15(net)\n",
    "        net = self.conv16(net)\n",
    "        net = self.maxPooling6(net)\n",
    "        net = self.dropOut(net)\n",
    "        net = self.flat(net)\n",
    "        net = self.dense1(net)\n",
    "        net = self.batchnorm(net)\n",
    "        net = self.drop(net)\n",
    "        net = self.dense2(net)\n",
    "        net = self.softmax(net)\n",
    "        return net\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(tf.__version__)\n",
    "    print(keras.__version__)\n",
    "\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            print('start with GPU 7')\n",
    "            tf.config.experimental.set_visible_devices(gpus[7], 'GPU')\n",
    "            tf.config.experimental.set_memory_growth(gpus[7], True)\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "\n",
    "    training_epochs = 250\n",
    "    batch_size = 128\n",
    "    learning_rate = 0.1\n",
    "    momentum = 0.9\n",
    "    lr_decay = 1e-6\n",
    "    lr_drop = 20\n",
    "\n",
    "    tf.random.set_seed(777)\n",
    "\n",
    "    def lr_scheduler(epoch):\n",
    "        return learning_rate * (0.5 ** (epoch // lr_drop))\n",
    "\n",
    "    reduce_lr = keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "    train_images, train_labels, test_images, test_labels = load_images()\n",
    "\n",
    "    # data augmentation\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(train_images)\n",
    "\n",
    "    model = VGG16Model()\n",
    "\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate,\n",
    "                                        decay=1e-6, momentum=momentum, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    model.fit_generator(datagen.flow(train_images, train_labels,\n",
    "                                     batch_size=batch_size), epochs=training_epochs, verbose=2, callbacks=[reduce_lr],\n",
    "                        steps_per_epoch=train_images.shape[0] // batch_size, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eb9cda",
   "metadata": {},
   "source": [
    "# SECOND IMPLEMENTATION FROM \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c1b074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Dense, Input, Conv2D, MaxPooling2D, Dropout\n",
    "from keras import optimizers\n",
    "cifar10 = datasets.cifar10 \n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog','frog', 'horse', 'ship', 'truck']\n",
    "#valid/test dataset 나누기\n",
    "valid_images = []\n",
    "valid_labels = []\n",
    "for i in range(5000):\n",
    "    valid_images.append(test_images[i])\n",
    "    valid_labels.append(test_labels[i])\n",
    "test_images_5000 = []\n",
    "test_labels_5000 = []\n",
    "for i in range(5000):\n",
    "    test_images_5000.append(test_images[5000+i])\n",
    "    test_labels_5000.append(test_labels[5000+i])\n",
    "\n",
    "test_images_5000 = np.array(test_images_5000)\n",
    "test_labels_5000 = np.array(test_labels_5000)\n",
    "valid_images = np.array(valid_images)\n",
    "valid_labels = np.array(valid_labels)\n",
    "print(\"Train samples:\", train_images.shape, train_labels.shape)\n",
    "print(\"Valid samples :\", valid_images.shape, valid_labels.shape)\n",
    "print(\"Test samples:\", test_images_5000.shape, test_labels_5000.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710355ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization\n",
    "def VGG16_Brief(classes=10): # classes = 감지할 클래스 수\n",
    "    img_rows, img_cols = 32, 32\n",
    "    img_channels = 3    \n",
    "    img_dim = (img_rows, img_cols, img_channels) #차원..shape 정의..\n",
    "    \n",
    "    img_input = Input(shape=img_dim) #튜플.. 변경 x\n",
    "    x = Conv2D(32,(3,3),padding='same',activation = 'relu')(img_input) #1층\n",
    "    x = Dropout(0.256)(x)\n",
    "    #x = Conv2D(32,(3,3),padding='same',activation = 'relu')(x) #2층\n",
    "    #x = Dropout(0.15)(x)\n",
    "    x = MaxPooling2D((2,2),strides=(2,2))(x) #2층 maxpool\n",
    "\n",
    "\n",
    "    x = Conv2D(64,(3,3),padding='same',activation = 'relu')(x) #3층\n",
    "    x = Dropout(0.25)(x)\n",
    "    #x = Conv2D(64,(3,3),padding='same',activation = 'relu')(x) #4층\n",
    "    #x = Dropout(0.15)(x)\n",
    "    x = MaxPooling2D((2,2),strides=(2,2))(x) #4층 maxpool\n",
    "    #x = Dropout(0.25)(x)\n",
    "\n",
    "    x = Conv2D(128,(3,3),padding='same',activation = 'relu')(x) #5층\n",
    "    x = Dropout(0.25)(x)\n",
    "    #x = Conv2D(256,(3,3),padding='same',activation = 'relu')(x) #6층\n",
    "   # x = Dropout(0.15)(x)\n",
    "    #x = Conv2D(256,(3,3),padding='same',activation = 'relu')(x) #7층\n",
    "    #x = Dropout(0.15)(x)\n",
    "    x = MaxPooling2D((2,2),strides=(2,2))(x) #7층 maxpool\n",
    "    #x = Dropout(0.2)(x)\n",
    "\n",
    "#     x = Conv2D(512,(3,3),padding='same',activation = 'relu')(x) #8층\n",
    "#     x = Conv2D(512,(3,3),padding='same',activation = 'relu')(x) #9층\n",
    "#     x = Conv2D(512,(3,3),padding='same',activation = 'relu')(x) #10층\n",
    "#     x = MaxPooling2D((2,2),strides=(2,2))(x) #10층 maxpool\n",
    "\n",
    "#     x = Conv2D(512,(3,3),padding='same',activation = 'relu')(x) #11층\n",
    "#     x = Conv2D(512,(3,3),padding='same',activation = 'relu')(x) #12층\n",
    "#     x = Conv2D(512,(3,3),padding='same',activation = 'relu')(x) #13층\n",
    "#     x = MaxPooling2D((2,2),strides=(2,2))(x) #13층 maxpool\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    #x = Dense(1024, activation = 'relu')(x) #14층\n",
    "    #x = Dropout(0.5)(x)\n",
    "    x = Dense(512, activation = 'relu')(x) #15층\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(classes, activation = 'softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=img_input, output = x)\n",
    "    return model\n",
    "\n",
    "model = VGG16_Brief(classes = 10)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480c6493",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', \n",
    "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#loss를 categorical_crossentropy 대신 sparse_categorical_crossentropy 사용\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            patience=5, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.1, \n",
    "                                            min_lr=0.00001)\n",
    "\n",
    "hist = model.fit(train_images, train_labels, epochs=100,\n",
    "          validation_data=(valid_images, valid_labels),callbacks = [learning_rate_reduction],batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36940e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images_5000,test_labels_5000)\n",
    "print('test loss:', test_loss)\n",
    "print('test acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e0061b",
   "metadata": {},
   "source": [
    "# DOWNLOAD WEIGHTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6096009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert the target directory exists\n",
    "path = \"./preload/CIFAR10/VGG16/\"\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "# Write the used inputs and outputs\n",
    "np.savetxt(fname=f\"{path}/input_0\", delimiter=\" \", X=images.cuda().view(-1, 3 * 33 * 33).tolist())\n",
    "np.savetxt(fname=f\"{path}/outputlayer8_0\", delimiter=\" \", X=model.output(images.cuda().view(-1, 3, 33, 33)).data.cpu().view(-1))\n",
    "\n",
    "# Write the layers we want\n",
    "for (i, resize_to) in enumerate([ (11*11*3, 96), (5*5*96, 256), (3*3*256, 384), (3*3*384, 384), (3*3*384, 256), None, None, None ]):\n",
    "    # Write the layer's weights\n",
    "    if resize_to is not None:\n",
    "        np.savetxt(fname=f\"{path}/weight{i + 1}_0\", delimiter=\" \", X=params[2 * i][1].reshape(resize_to).tolist())\n",
    "    else:\n",
    "        np.savetxt(fname=f\"{path}/weight{i + 1}_0\", delimiter=\" \", X=params[2 * i][1].tolist())\n",
    "\n",
    "    # Write the biases\n",
    "    np.savetxt(fname=f\"{path}/bias{i + 1}_0\", delimiter=\" \", X=params[2 * i + 1][1].tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
