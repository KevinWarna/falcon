{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6ae1b9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70be5f5",
   "metadata": {},
   "source": [
    "# upload datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0f36bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "train_set = dset.MNIST(root='./data', train=True, transform=trans, download=True)\n",
    "test_set = dset.MNIST(root='./data', train=False, transform=trans)\n",
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set,batch_size=batch_size,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set,batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08e4f74",
   "metadata": {},
   "source": [
    "# define neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3308d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):   \n",
    "    def __init__(self, num=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True), \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),   \n",
    "            nn.MaxPool2d( kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 96, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),                         \n",
    "            nn.Conv2d(96, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),                         \n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d( kernel_size=2, stride=1),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(32*12*12,2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(2048,1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024,num),\n",
    "         \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.feature(x)\n",
    "        x = x.view(-1,32*12*12)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fd206f",
   "metadata": {},
   "source": [
    "# define train and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d99860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "#         loss = F.nll_loss(output, target)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        train_losses.append(loss.data[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('\\rEpoch: {} {:.0f}%\\t     Loss: {:.6f}'.format(\n",
    "                epoch,\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]), end='')\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.cross_entropy(output, target, size_average=False)\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    acc=100. * float(correct.to(torch.device('cpu')).numpy())\n",
    "    print('\\nTest result: Average loss: {:.4f}, Accuracy: {:.4f}%\\n'.format(\n",
    "        test_loss, acc / len(test_loader.dataset)))\n",
    "    \n",
    "    test_accuracy.append(acc / len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9a7884",
   "metadata": {},
   "source": [
    "# load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8567e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AlexNet()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9437a845",
   "metadata": {},
   "source": [
    "# check if gpu is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f687bdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3088c5",
   "metadata": {},
   "source": [
    "# define learning rate of stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b648b4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e7a96e",
   "metadata": {},
   "source": [
    "# Perform TRAINING and TESTING iteratively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "748ab5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f6/8cblqgm51_l7vc0l47h_c2bw0000gn/T/ipykernel_34309/1755829714.py:26: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test result: Average loss: 2.3027, Accuracy: 10.3200%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train_losses = []\n",
    "test_losses =[]\n",
    "test_accuracy = []\n",
    "# for epoch in range(1, 15):\n",
    "#     train(epoch)\n",
    "#     test()\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd47cd60",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# FOR C++\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# .\n",
    "# set path to folder where the weights should be downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaef497b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/daphneechabal/Library/CloudStorage/OneDrive-UvA/WORK/2.COMPARATIVE PAPER/falcon/scripts/MNIST_AlexNet'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f51acc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../files/preload/MNIST/AlexNet/\"\n",
    "if not os.path.exists(path):\n",
    "    print(\"THE OUTPUT FILE FOR THE WEIGHTS OF ALEXNET ON MNIST DOES NOT EXIST in /preload.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9759c842",
   "metadata": {},
   "source": [
    "# Take the very first test image and create the input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "56d0ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_input = test_set[0][0].detach().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0f0db2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    np.savetxt(fname=f\"{path}/input_0_{i}\", delimiter=\" \", X=flat_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8793df",
   "metadata": {},
   "source": [
    "# TAKE THE WEIGHTS AND BIAS OF THE REMAINING LAYERS WHICH ARE NOT RELU NOT MAXPOOL (only the layers that have weights) and create files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4be388",
   "metadata": {},
   "source": [
    "### Display layers first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7edc662b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of AlexNet(\n",
       "  (feature): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(96, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (10): ReLU(inplace=True)\n",
       "    (11): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=4608, out_features=2048, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=1024, out_features=10, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64978b90",
   "metadata": {},
   "source": [
    "### create a file for each layer:\n",
    "# remember: for i in range (3) is the generate identical files for each parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e3b5f0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    for name, param in model.named_parameters():\n",
    "        n = name.split(\".\")\n",
    "        if n[0] == \"classifier\":\n",
    "            n[1] = int(n[1])+12\n",
    "        new_name = n[2] + \"_\" + str(n[1]) + \"_\" + str(i)\n",
    "        paramnp = param.detach().numpy().flatten()\n",
    "        np.savetxt(fname=f\"{path}/{new_name}\", delimiter=\" \", X=paramnp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30874e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
